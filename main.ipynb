{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13f162b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import pdb\n",
    "import os\n",
    "import math\n",
    "from random import Random\n",
    "\n",
    "# internal imports\n",
    "from utils.file_utils import save_pkl, load_pkl\n",
    "from utils.utils import *\n",
    "from utils.core_utils import train\n",
    "from datasets.dataset_generic import Generic_WSI_Classification_Dataset, Generic_MIL_Dataset\n",
    "\n",
    "# pytorch imports\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, sampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e237be9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create splits\n",
    "\n",
    "seed = 10\n",
    "patch_dir = \"image_sets/patches/\"\n",
    "dest_dir = \"image_sets/splits/\"\n",
    "    \n",
    "for folder in [dest_dir, dest_dir+\"train\", dest_dir+\"test\", dest_dir+\"val\"]:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "# [train, test, val]\n",
    "split_ratios = [0.76, 0.12, 0.12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2a2a3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141 108 17 16\n"
     ]
    }
   ],
   "source": [
    "patches_list = sorted(os.listdir(patch_dir))\n",
    "# print(patches_list)\n",
    "Random(seed).shuffle(patches_list)\n",
    "# print(patches_list)\n",
    "\n",
    "patches_list_length = len(patches_list)\n",
    "train_length = math.ceil(split_ratios[0] * patches_list_length)\n",
    "test_length = math.ceil(split_ratios[1] * patches_list_length)\n",
    "val_length = patches_list_length - train_length - test_length\n",
    "print(patches_list_length, train_length, test_length, val_length)\n",
    "\n",
    "train_dataset = patches_list[:train_length]\n",
    "test_dataset = patches_list[train_length:train_length + test_length]\n",
    "val_dataset = patches_list[train_length + test_length:]\n",
    "# print(len(train), len(test), len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f3913bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic training settings\n",
    "# Configurations for WSI Training\n",
    "\n",
    "data_root_dir = \"image_sets/patches/\"\n",
    "max_epochs = 200\n",
    "lr = 1e-4\n",
    "label_frac = 1.0\n",
    "reg = 1e-5\n",
    "seed = 1\n",
    "k = 10\n",
    "k_start = -1\n",
    "k_end = -1\n",
    "results_dir = \"image_sets/results\"\n",
    "split_dir = \"fungal_vs_nonfungal_100\"\n",
    "log_data = False\n",
    "testing = False\n",
    "early_stopping = False\n",
    "opt = 'adam'\n",
    "drop_out = False\n",
    "bag_loss = 'ce'\n",
    "model_type = 'clam_sb'\n",
    "weighted_sample = False\n",
    "model_size = 'small'\n",
    "task = 'task_fungal_vs_nonfungal'\n",
    "### CLAM specific options\n",
    "no_inst_cluster = False\n",
    "inst_loss = None\n",
    "subtyping = False\n",
    "bag_weight = 0.7\n",
    "B = 8\n",
    "\n",
    "exp_code = \"exp_0\"\n",
    "\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01b27446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load Dataset\n",
      "    case_id   slide_id label\n",
      "0    case_0    slide_0     1\n",
      "1    case_0    slide_1     0\n",
      "2    case_0    slide_2     0\n",
      "3    case_0    slide_3     0\n",
      "4    case_0    slide_4     1\n",
      "..      ...        ...   ...\n",
      "135  case_9  slide_135     1\n",
      "136  case_9  slide_136     1\n",
      "137  case_9  slide_137     1\n",
      "138  case_9  slide_138     1\n",
      "139  case_9  slide_139     1\n",
      "\n",
      "[140 rows x 3 columns]\n",
      "label column: label\n",
      "label dictionary: {'nonfungal': 0, 'fungal': 1}\n",
      "number of classes: 2\n",
      "slide-level counts:  \n",
      " 0      3\n",
      "1    137\n",
      "Name: label, dtype: int64\n",
      "Patient-LVL; Number of samples registered in class 0: 0\n",
      "Slide-LVL; Number of samples registered in class 0: 3\n",
      "Patient-LVL; Number of samples registered in class 1: 10\n",
      "Slide-LVL; Number of samples registered in class 1: 137\n",
      "################# Settings ###################\n",
      "num_splits:  10\n",
      "k_start:  -1\n",
      "k_end:  -1\n",
      "task:  task_fungal_vs_nonfungal\n",
      "max_epochs:  200\n",
      "results_dir:  image_sets/results\n",
      "lr:  0.0001\n",
      "experiment:  exp_0\n",
      "reg:  1e-05\n",
      "label_frac:  1.0\n",
      "bag_loss:  ce\n",
      "seed:  1\n",
      "model_type:  clam_sb\n",
      "model_size:  small\n",
      "use_drop_out:  False\n",
      "weighted_sample:  False\n",
      "opt:  adam\n",
      "data_root_dir:  None\n",
      "k:  10\n",
      "split_dir:  None\n",
      "log_data:  False\n",
      "testing:  False\n",
      "early_stopping:  False\n",
      "dropout:  False\n",
      "no_inst_cluster:  False\n",
      "inst_loss:  None\n",
      "subtyping:  False\n",
      "bag_weight:  0.7\n",
      "B:  8\n"
     ]
    }
   ],
   "source": [
    "def seed_torch(seed=7):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "seed_torch(seed)\n",
    "\n",
    "encoding_size = 1024\n",
    "settings = {'num_splits': k,\n",
    "            'k_start': k_start,\n",
    "            'k_end': k_end,\n",
    "            'task': task,\n",
    "            'max_epochs': max_epochs,\n",
    "            'results_dir': results_dir,\n",
    "            'lr': lr,\n",
    "            'experiment': exp_code,\n",
    "            'reg': reg,\n",
    "            'label_frac': label_frac,\n",
    "            'bag_loss': bag_loss,\n",
    "            'seed': seed,\n",
    "            'model_type': model_type,\n",
    "            'model_size': model_size,\n",
    "            \"use_drop_out\": drop_out,\n",
    "            'weighted_sample': weighted_sample,\n",
    "            'opt': opt,\n",
    "            'data_root_dir': None,\n",
    "            'label_frac': 1.0,\n",
    "            'k': 10,\n",
    "            'split_dir': None,\n",
    "            'log_data': False,\n",
    "            'testing': False,\n",
    "            'early_stopping': False,\n",
    "            'dropout': False,\n",
    "            'no_inst_cluster': False,\n",
    "            'inst_loss': None,\n",
    "            'subtyping': False,\n",
    "            'bag_weight': 0.7,\n",
    "            'B': 8\n",
    "            }\n",
    "\n",
    "if model_type in ['clam_sb', 'clam_mb']:\n",
    "    settings.update({'bag_weight': bag_weight,\n",
    "                     'inst_loss': inst_loss,\n",
    "                     'B': B})\n",
    "\n",
    "print('\\nLoad Dataset')\n",
    "\n",
    "\n",
    "if task == 'task_fungal_vs_nonfungal':\n",
    "    n_classes = 2\n",
    "    dataset = Generic_MIL_Dataset(csv_path='dataset_csv/fungal_vs_nonfungal.csv',\n",
    "                                  data_dir=os.path.join(\n",
    "                                      data_root_dir, 'fungal_vs_nonfungal_resnet_features'),\n",
    "                                  shuffle=False,\n",
    "                                  seed=seed,\n",
    "                                  print_info=True,\n",
    "                                  label_dict={'nonfungal': 0, 'fungal': 1},\n",
    "                                  patient_strat=False,\n",
    "                                  ignore=[])\n",
    "\n",
    "elif task == 'task_1_tumor_vs_normal':\n",
    "    n_classes = 2\n",
    "    dataset = Generic_MIL_Dataset(csv_path='dataset_csv/tumor_vs_normal_dummy_clean.csv',\n",
    "                                  data_dir=os.path.join(\n",
    "                                      data_root_dir, 'tumor_vs_normal_resnet_features'),\n",
    "                                  shuffle=False,\n",
    "                                  seed=seed,\n",
    "                                  print_info=True,\n",
    "                                  label_dict={'normal_tissue': 0,\n",
    "                                              'tumor_tissue': 1},\n",
    "                                  patient_strat=False,\n",
    "                                  ignore=[])\n",
    "\n",
    "elif task == 'task_2_tumor_subtyping':\n",
    "    n_classes = 3\n",
    "    dataset = Generic_MIL_Dataset(csv_path='dataset_csv/tumor_subtyping_dummy_clean.csv',\n",
    "                                  data_dir=os.path.join(\n",
    "                                      data_root_dir, 'tumor_subtyping_resnet_features'),\n",
    "                                  shuffle=False,\n",
    "                                  seed=seed,\n",
    "                                  print_info=True,\n",
    "                                  label_dict={'subtype_1': 0,\n",
    "                                              'subtype_2': 1, 'subtype_3': 2},\n",
    "                                  patient_strat=False,\n",
    "                                  ignore=[])\n",
    "\n",
    "    if model_type in ['clam_sb', 'clam_mb']:\n",
    "        assert subtyping\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "if not os.path.isdir(results_dir):\n",
    "    os.mkdir(results_dir)\n",
    "\n",
    "results_dir = os.path.join(results_dir, str(exp_code) + '_s{}'.format(seed))\n",
    "if not os.path.isdir(results_dir):\n",
    "    os.mkdir(results_dir)\n",
    "\n",
    "if split_dir is None:\n",
    "    split_dir = os.path.join('splits', task+'_{}'.format(int(label_frac*100)))\n",
    "else:\n",
    "    split_dir = os.path.join('splits', split_dir)\n",
    "\n",
    "# print('split_dir: ', split_dir)\n",
    "# assert os.path.isdir(split_dir)\n",
    "\n",
    "# settings.update({'split_dir': split_dir})\n",
    "\n",
    "\n",
    "with open(results_dir + '/experiment_{}.txt'.format(exp_code), 'w') as f:\n",
    "    print(settings, file=f)\n",
    "f.close()\n",
    "\n",
    "print(\"################# Settings ###################\")\n",
    "for key, val in settings.items():\n",
    "    print(\"{}:  {}\".format(key, val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0445d4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'num_splits': 10, 'k_start': -1, 'k_end': -1, 'task': 'task_fungal_vs_nonfungal', 'max_epochs': 200, 'results_dir': 'image_sets/results', 'lr': 0.0001, 'experiment': 'exp_0', 'reg': 1e-05, 'label_frac': 1.0, 'bag_loss': 'ce', 'seed': 1, 'model_type': 'clam_sb', 'model_size': 'small', 'use_drop_out': False, 'weighted_sample': False, 'opt': 'adam', 'data_root_dir': None, 'k': 10, 'split_dir': None, 'log_data': False, 'testing': False, 'early_stopping': False, 'dropout': False, 'no_inst_cluster': False, 'inst_loss': None, 'subtyping': False, 'bag_weight': 0.7, 'B': 8}\n",
      "\n",
      "Training Fold 0!\n",
      "\n",
      "Init train/val/test splits... (['F007a13', 'F030a15', 'F010a01', 'F033a03', 'F033a14', 'F034a09', 'F007a11', 'F009a02', 'F007a16', 'F030a01', 'F007a06', 'F007a03', 'F018a02', 'F013a04', 'F033a11', 'F030a04', 'F013a13', 'F012a03', 'F030a09', 'F007a08', 'F013a08', 'F009a01', 'F018a11', 'F006a02', 'F012a01', 'F033a26', 'F033a09', 'F006a01', 'F009a04', 'F018a09', 'F033a21', 'F007a04', 'F018a05', 'F034a07', 'F033a12', 'F018a10', 'F010a03', 'F030a14', 'F030a12', 'F034a05', 'F007a21', 'F030a11', 'F017a01', 'F007a01', 'F033a07', 'F010a05', 'F017a09', 'F030a05', 'F012a02', 'F007a05', 'F033a01', 'F007a17', 'F007a18', 'F034a11', 'F007a02', 'F013a05', 'F017a07', 'F018a06', 'F006a06', 'F018a12', 'F017a05', 'F015a01', 'F006a07', 'F017a06', 'F013a01', 'F013a03', 'F033a25', 'F006a10', 'F030a02', 'F006a09', 'F030a03', 'F033a24', 'F018a03', 'F006a04', 'F021a05', 'F034a08', 'F007a19', 'F007a10', 'F017a04', 'F033a29', 'F030a08', 'F033a28', 'F013a15', 'F013a09', 'F013a11', 'F017a10', 'F021a02', 'F011a01', 'F030a06', 'F033a10', 'F017a08', 'F033a02', 'F007a15', 'F030a17', 'F030a13', 'F005a02', 'F018a04', 'F033a19', 'F033a13', 'F018a08', 'F013a10', 'F007a22', 'F030a10', 'F033a17', 'F033a16', 'F034a10', 'F021a01', 'F010a04'], ['F033a23', 'F033a18', 'F017a02', 'F007a09', 'F018a13', 'F034a06', 'fungal_vs_nonfungal_resnet_features', 'F011a02', 'F018a01', 'F033a22', 'F033a15', 'F013a06', 'F006a03', 'F033a20', 'F033a04', 'F006a08'], ['F021a04', 'F007a12', 'F013a12', 'F009a03', 'F021a03', 'F030a18', 'F010a02', 'F033a27', 'F013a02', 'F012a04', 'F018a07', 'F007a07', 'F033a05', 'F013a07', 'F006a05', 'F012a06', 'F030a07'])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'slide_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7210/3240624665.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m#     datasets = (train_dataset, val_dataset, test_dataset)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_auc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_auc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mall_test_auc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_auc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mall_val_auc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_auc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RajithaKV/ROI_Detection/CLAM_model/CLAM_1/utils/core_utils.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(datasets, cur, settings)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nInit train/val/test splits...'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mtrain_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0msave_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'results_dir'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'splits_{}.csv'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training on {} samples\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RajithaKV/ROI_Detection/CLAM_model/CLAM_1/datasets/dataset_generic.py\u001b[0m in \u001b[0;36msave_splits\u001b[0;34m(split_datasets, column_keys, filename, boolean_style)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msave_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_datasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboolean_style\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_datasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0msplits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msplit_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslide_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'slide_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_datasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mboolean_style\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RajithaKV/ROI_Detection/CLAM_model/CLAM_1/datasets/dataset_generic.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msave_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_datasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboolean_style\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_datasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0msplits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msplit_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslide_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'slide_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_datasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mboolean_style\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'slide_data'"
     ]
    }
   ],
   "source": [
    "# main\n",
    "\n",
    "# create results directory if necessary\n",
    "if not os.path.isdir(results_dir):\n",
    "    os.mkdir(results_dir)\n",
    "\n",
    "if k_start == -1:\n",
    "    start = 0\n",
    "else:\n",
    "    start = k_start\n",
    "if k_end == -1:\n",
    "    end = k\n",
    "else:\n",
    "    end = k_end\n",
    "\n",
    "all_test_auc = []\n",
    "all_val_auc = []\n",
    "all_test_acc = []\n",
    "all_val_acc = []\n",
    "folds = np.arange(start, end)\n",
    "for i in folds:\n",
    "    seed_torch(seed)\n",
    "    train_dataset, val_dataset, test_dataset = dataset.return_splits(from_id=False, \n",
    "            csv_path='{}/splits_{}.csv'.format(split_dir, i))\n",
    "    \n",
    "#     datasets = (train_dataset, val_dataset, test_dataset)\n",
    "    \n",
    "    results, test_auc, val_auc, test_acc, val_acc  = train(datasets, i, settings)\n",
    "    all_test_auc.append(test_auc)\n",
    "    all_val_auc.append(val_auc)\n",
    "    all_test_acc.append(test_acc)\n",
    "    all_val_acc.append(val_acc)\n",
    "    #write results to pkl\n",
    "    filename = os.path.join(results_dir, 'split_{}_results.pkl'.format(i))\n",
    "    save_pkl(filename, results)\n",
    "\n",
    "final_df = pd.DataFrame({'folds': folds, 'test_auc': all_test_auc, \n",
    "    'val_auc': all_val_auc, 'test_acc': all_test_acc, 'val_acc' : all_val_acc})\n",
    "\n",
    "if len(folds) != k:\n",
    "    save_name = 'summary_partial_{}_{}.csv'.format(start, end)\n",
    "else:\n",
    "    save_name = 'summary.csv'\n",
    "final_df.to_csv(os.path.join(results_dir, save_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9092e6ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clam",
   "language": "python",
   "name": "clam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
