{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a17fe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import h5py\n",
    "\n",
    "from models.resnet_custom import resnet50_baseline\n",
    "from utils.utils import print_network, collate_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "999b8835",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_dir = \"image_sets/patches/\"\n",
    "feat_dir = \"image_sets/features/\"\n",
    "actual_feat_dir = \"image_sets/patches/fungal_vs_nonfungal_resnet_features/pt_files/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98af23e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features directory doesn't exist. Creating ...\n",
      "ERROR: Cannot create the Features directory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet_Baseline(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck_Baseline(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck_Baseline(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck_Baseline(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck_Baseline(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck_Baseline(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck_Baseline(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck_Baseline(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck_Baseline(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck_Baseline(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck_Baseline(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck_Baseline(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck_Baseline(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck_Baseline(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Create feat_dir if not exists. \n",
    "# Not properly fixed\n",
    "if not os.path.exists(feat_dir):\n",
    "    try:\n",
    "        print(\"Features directory doesn't exist. Creating ...\")\n",
    "        os.mkdir(feat_dir, exist_ok=True)\n",
    "    except:\n",
    "        print(\"ERROR: Cannot create the Features directory\")\n",
    "\n",
    "model = resnet50_baseline(pretrained=True)\n",
    "model = model.to(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "351cd4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset from the image patches\n",
    "dataset = []\n",
    "for folder in sorted(os.listdir(patch_dir)):\n",
    "    patch_folder = os.path.join(patch_dir, folder)\n",
    "    for patch_file in sorted(os.listdir(patch_folder)):\n",
    "        if patch_file == \"pt_files\":\n",
    "            continue\n",
    "        \n",
    "        img_path = os.path.join(patch_folder, patch_file)\n",
    "\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        img_arr = np.asarray(img)\n",
    "        # img_arr = np.expand_dims(img_arr, 0)\n",
    "        # img_PIL = Image.fromarray(img_arr)\n",
    "\n",
    "        # Create the dataset loader\n",
    "        imgs = torch.tensor(img_arr)\n",
    "\n",
    "        # Get coord in [x, y] format\n",
    "        coord = img_path.split(\"/\")\n",
    "        coord = coord[-1]\n",
    "        coord = coord.split(\".\")[-2]\n",
    "        coord = coord.split(\"_\")\n",
    "        coord = [int(coord[-2])/256, int(coord[-1])/256]\n",
    "\n",
    "        dataset.append([imgs, coord])\n",
    "\n",
    "loader = DataLoader(dataset=dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "266a5eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['image_sets/patches/F005a02', 'image_sets/patches/F006a01', 'image_sets/patches/F006a02', 'image_sets/patches/F006a03', 'image_sets/patches/F006a04', 'image_sets/patches/F006a05', 'image_sets/patches/F006a06', 'image_sets/patches/F006a07', 'image_sets/patches/F006a08', 'image_sets/patches/F006a09', 'image_sets/patches/F006a10', 'image_sets/patches/F007a01', 'image_sets/patches/F007a02', 'image_sets/patches/F007a03', 'image_sets/patches/F007a04', 'image_sets/patches/F007a05', 'image_sets/patches/F007a06', 'image_sets/patches/F007a07', 'image_sets/patches/F007a08', 'image_sets/patches/F007a09', 'image_sets/patches/F007a10', 'image_sets/patches/F007a11', 'image_sets/patches/F007a12', 'image_sets/patches/F007a13', 'image_sets/patches/F007a15', 'image_sets/patches/F007a16', 'image_sets/patches/F007a17', 'image_sets/patches/F007a18', 'image_sets/patches/F007a19', 'image_sets/patches/F007a21', 'image_sets/patches/F007a22', 'image_sets/patches/F009a01', 'image_sets/patches/F009a02', 'image_sets/patches/F009a03', 'image_sets/patches/F009a04', 'image_sets/patches/F010a01', 'image_sets/patches/F010a02', 'image_sets/patches/F010a03', 'image_sets/patches/F010a04', 'image_sets/patches/F010a05', 'image_sets/patches/F011a01', 'image_sets/patches/F011a02', 'image_sets/patches/F012a01', 'image_sets/patches/F012a02', 'image_sets/patches/F012a03', 'image_sets/patches/F012a04', 'image_sets/patches/F012a06', 'image_sets/patches/F013a01', 'image_sets/patches/F013a02', 'image_sets/patches/F013a03', 'image_sets/patches/F013a04', 'image_sets/patches/F013a05', 'image_sets/patches/F013a06', 'image_sets/patches/F013a07', 'image_sets/patches/F013a08', 'image_sets/patches/F013a09', 'image_sets/patches/F013a10', 'image_sets/patches/F013a11', 'image_sets/patches/F013a12', 'image_sets/patches/F013a13', 'image_sets/patches/F013a15', 'image_sets/patches/F015a01', 'image_sets/patches/F017a01', 'image_sets/patches/F017a02', 'image_sets/patches/F017a04', 'image_sets/patches/F017a05', 'image_sets/patches/F017a06', 'image_sets/patches/F017a07', 'image_sets/patches/F017a08', 'image_sets/patches/F017a09', 'image_sets/patches/F017a10', 'image_sets/patches/F018a01', 'image_sets/patches/F018a02', 'image_sets/patches/F018a03', 'image_sets/patches/F018a04', 'image_sets/patches/F018a05', 'image_sets/patches/F018a06', 'image_sets/patches/F018a07', 'image_sets/patches/F018a08', 'image_sets/patches/F018a09', 'image_sets/patches/F018a10', 'image_sets/patches/F018a11', 'image_sets/patches/F018a12', 'image_sets/patches/F018a13', 'image_sets/patches/F021a01', 'image_sets/patches/F021a02', 'image_sets/patches/F021a03', 'image_sets/patches/F021a04', 'image_sets/patches/F021a05', 'image_sets/patches/F030a01', 'image_sets/patches/F030a02', 'image_sets/patches/F030a03', 'image_sets/patches/F030a04', 'image_sets/patches/F030a05', 'image_sets/patches/F030a06', 'image_sets/patches/F030a07', 'image_sets/patches/F030a08', 'image_sets/patches/F030a09', 'image_sets/patches/F030a10', 'image_sets/patches/F030a11', 'image_sets/patches/F030a12', 'image_sets/patches/F030a13', 'image_sets/patches/F030a14', 'image_sets/patches/F030a15', 'image_sets/patches/F030a17', 'image_sets/patches/F030a18', 'image_sets/patches/F033a01', 'image_sets/patches/F033a02', 'image_sets/patches/F033a03', 'image_sets/patches/F033a04', 'image_sets/patches/F033a05', 'image_sets/patches/F033a07', 'image_sets/patches/F033a09', 'image_sets/patches/F033a10', 'image_sets/patches/F033a11', 'image_sets/patches/F033a12', 'image_sets/patches/F033a13', 'image_sets/patches/F033a14', 'image_sets/patches/F033a15', 'image_sets/patches/F033a16', 'image_sets/patches/F033a17', 'image_sets/patches/F033a18', 'image_sets/patches/F033a19', 'image_sets/patches/F033a20', 'image_sets/patches/F033a21', 'image_sets/patches/F033a22', 'image_sets/patches/F033a23', 'image_sets/patches/F033a24', 'image_sets/patches/F033a25', 'image_sets/patches/F033a26', 'image_sets/patches/F033a27', 'image_sets/patches/F033a28', 'image_sets/patches/F033a29', 'image_sets/patches/F034a05', 'image_sets/patches/F034a06', 'image_sets/patches/F034a07', 'image_sets/patches/F034a08', 'image_sets/patches/F034a09', 'image_sets/patches/F034a10', 'image_sets/patches/F034a11', 'image_sets/patches/fungal_vs_nonfungal_resnet_features']\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "patch_folders = [os.path.join(patch_dir, folder) for folder in sorted(os.listdir(patch_dir))]\n",
    "print(patch_folders)\n",
    "patches_per_image = len(patch_folders[0])\n",
    "print(patches_per_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82be4dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F005a02\n",
      "0  ||  [tensor([0.], dtype=torch.float64), tensor([0.], dtype=torch.float64)]  ||  tensor([[ 0.4815,  0.0000,  2.4632,  ..., 30.7341,  0.0000,  0.0000]])  ||  image_sets/features/F005a02.pt\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'image_sets/features/F005a02.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_47042/2267860145.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" || \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" || \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" || \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilePath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# print(\"Features size: \", features.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilePath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Save the .hdf5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/clam/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \"\"\"\n\u001b[0;32m--> 260\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/clam/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_with_file_like\u001b[0;34m(f, mode, body)\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'image_sets/features/F005a02.pt'"
     ]
    }
   ],
   "source": [
    "for count, data in enumerate(loader):\n",
    "    with torch.no_grad():\n",
    "        filename = str(patch_folders[count//patches_per_image])\n",
    "        filename = filename.split(\"/\")[-1]\n",
    "        coord = data[1]\n",
    "        batch = data[0]\n",
    "        batch = torch.unsqueeze(batch, 0)\n",
    "        batch = batch.reshape([1, 3, 256, 256])\n",
    "        batch = batch.to(device, non_blocking=True)\n",
    "        batch = batch.float()\n",
    "\n",
    "        features = model(batch)\n",
    "        features = features.cpu().numpy()\n",
    "        features = torch.from_numpy(features)\n",
    "\n",
    "        print(filename)\n",
    "        filePath = os.path.join(feat_dir, filename+'.pt')\n",
    "        print(count, \" || \", coord, \" || \", features, \" || \", filePath)\n",
    "        # print(\"Features size: \", features.shape)\n",
    "        torch.save(features, filePath)\n",
    "\n",
    "        # Save the .hdf5\n",
    "        # hf = h5py.File('data.h5', 'w')\n",
    "\n",
    "        print(\"=\"*15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f634a4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_feat_dir = actual_feat_dir\n",
    "feat_dir = feat_dir\n",
    "!mkdir -p $actual_feat_dir\n",
    "!mv $feat_dir/* $actual_feat_dir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clam",
   "language": "python",
   "name": "clam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
